{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('training_cleaned.csv')\n",
    "data_test = pd.read_csv('test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert categorical data to dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = list(data_train.select_dtypes(exclude=[object]).columns)\n",
    "data_cat = list(data_train.select_dtypes(include=[object]).columns)\n",
    "data_train = pd.get_dummies(data_train, columns=data_cat)\n",
    "data_test = pd.get_dummies(data_test, columns=data_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making columns same in all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_cols = list(set(list(data_train.columns)+list(data_test.columns)))\n",
    "\n",
    "for i in com_cols:\n",
    "    if i not in data_train.columns:\n",
    "        data_train[i] = 0\n",
    "    elif i not in data_test.columns:\n",
    "        data_test[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log Tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in data_int:\n",
    "    if j != 'SalePrice':\n",
    "        data_train[j] = np.log1p(data_train[j])\n",
    "        data_test[j] = np.log1p(data_test[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test and Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete train data\n",
    "x_train_complete = data_train[data_train.columns[~data_train.columns.isin(['SalePrice'])]]\n",
    "y_train_complete = data_train[['SalePrice']]\n",
    "\n",
    "# partial train data \n",
    "x_train = x_train_complete.iloc[:1201,:]\n",
    "y_train = y_train_complete.iloc[:1201,:]\n",
    "\n",
    "# validation data\n",
    "x_val = x_train_complete.iloc[1201:,:]\n",
    "y_val = y_train_complete.iloc[1201:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_complete.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log1p(y_train_complete).plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step will take lot of time\n",
    "mod = RandomForestRegressor(random_state = 42)\n",
    "rfecv = RFECV(estimator=mod, step=1, cv=5, scoring='neg_mean_absolute_error')\n",
    "rfecv.fit(x_train_complete,y_train_complete)\n",
    "features = list(x_train_complete.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_xgb = {'min_child_weight': [1],\n",
    "#               'gamma': [0.5],\n",
    "#               'subsample': [0.8,1.0],\n",
    "#               'colsample_bytree': [0.5,0.7],\n",
    "#               'max_depth': [3],\n",
    "#               'learning_rate':[0.01,0.05,0.1],\n",
    "#               'n_estimators':[100,200,500],\n",
    "#               'random_state':[0]}\n",
    "\n",
    "params_xgb = {'min_child_weight': [1],\n",
    "              'gamma': [0.5],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'max_depth': [3],\n",
    "              'learning_rate':[0.01],\n",
    "              'n_estimators':[5000],\n",
    "              'random_state':[0]}\n",
    "\n",
    "# partial data\n",
    "model_xgb = xgboost.XGBRegressor() \n",
    "xgb_model = GridSearchCV(model_xgb, params_xgb, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "xgb_model.fit(x_train[features].values,np.log1p(y_train))\n",
    "\n",
    "# complete data\n",
    "model_xgb_complete = xgboost.XGBRegressor()\n",
    "xgb_model_complete = GridSearchCV(model_xgb_complete, params_xgb, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "xgb_model_complete.fit(x_train_complete[features].values,np.log1p(y_train_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_gb = { 'loss':['ls'],\n",
    "#               'subsample': [0.5],\n",
    "#               'max_depth': [3],\n",
    "#               'learning_rate':[0.05],\n",
    "#               'n_estimators':[100],\n",
    "#               'random_state':[0]}\n",
    "\n",
    "params_gb = { 'loss':['ls'],\n",
    "              'subsample': [0.7],\n",
    "              'max_depth': [3],\n",
    "              'learning_rate':[0.01],\n",
    "              'n_estimators':[5000],\n",
    "              'random_state':[0]}\n",
    "\n",
    "# partial data\n",
    "model_gb = GradientBoostingRegressor()\n",
    "gb_model = GridSearchCV(model_gb, params_gb, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "gb_model.fit(x_train[features].values,np.log1p(y_train))\n",
    "\n",
    "\n",
    "# complete data\n",
    "model_gb_complete = GradientBoostingRegressor()\n",
    "gb_model_complete = GridSearchCV(model_gb_complete, params_gb, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "gb_model_complete.fit(x_train_complete[features].values,np.log1p(y_train_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "              'max_leaf_nodes': [5],\n",
    "              'max_depth': [3],\n",
    "              'n_estimators':[5000],\n",
    "              'random_state':[0]}\n",
    "\n",
    "# partial data\n",
    "model_rf = RandomForestRegressor() \n",
    "rf_model = GridSearchCV(model_rf, params_rf, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "rf_model.fit(x_train[features].values,np.log1p(y_train))\n",
    "\n",
    "# complete data\n",
    "model_rf_complete = RandomForestRegressor()\n",
    "rf_model_complete = GridSearchCV(model_rf_complete, params_rf, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "rf_model_complete.fit(x_train_complete[features].values,np.log1p(y_train_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dt = {\n",
    "                'random_state':[0]}\n",
    "\n",
    "# partial data\n",
    "model_dt = DecisionTreeRegressor()\n",
    "dt_model = GridSearchCV(model_dt, params_dt, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "dt_model.fit(x_train[features].values,np.log1p(y_train))\n",
    "\n",
    "# complete data\n",
    "model_dt_complete = DecisionTreeRegressor()\n",
    "dt_model_complete = GridSearchCV(model_dt_complete, params_dt, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "dt_model_complete.fit(x_train_complete[features].values,np.log1p(y_train_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ls = {\n",
    "                'alpha': [0.5],\n",
    "                'max_iter':[11500],\n",
    "                'random_state':[0],\n",
    "                }\n",
    "\n",
    "# partial data\n",
    "model_ls = Lasso() \n",
    "ls_model = GridSearchCV(model_ls, params_ls, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "ls_model.fit(x_train[features].values,np.log1p(y_train))\n",
    "\n",
    "# complete data\n",
    "model_ls_complete = Lasso() \n",
    "ls_model_complete = GridSearchCV(model_ls_complete, params_ls, n_jobs=-1, cv=5, refit=True, scoring = 'neg_mean_absolute_error')\n",
    "ls_model_complete.fit(x_train_complete[features].values,np.log1p(y_train_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_p = x_val.copy()\n",
    "x_val_p['act'] = y_val\n",
    "pred_xgb = xgb_model.predict(x_val[features].values)\n",
    "x_val_p['pred_xgb'] = np.expm1(pred_xgb)\n",
    "pred_gb = gb_model.predict(x_val[features].values)\n",
    "x_val_p['pred_gb'] = np.expm1(pred_gb)\n",
    "pred_rf = rf_model.predict(x_val[features].values)\n",
    "x_val_p['pred_rf'] = np.expm1(pred_rf)\n",
    "pred_dt = dt_model.predict(x_val[features].values)\n",
    "x_val_p['pred_dt'] = np.expm1(pred_dt)\n",
    "pred_ls = ls_model.predict(x_val[features].values)\n",
    "x_val_p['pred_ls'] = np.expm1(pred_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE represented as percentage.The lowest MAPE was from Gradient Boost Model.\n",
    "-- MAPE on Validation Data – 8.27%  \n",
    "\n",
    "-- MAPE on Test Data – 8.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_xgb_p = round((np.mean(np.abs(x_val_p['pred_xgb'] - x_val_p['act']) / x_val_p['act']))*100,2)\n",
    "mape_xgb_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boost MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_gb_p = round((np.mean(np.abs(x_val_p['pred_gb'] - x_val_p['act']) / x_val_p['act']))*100,2)\n",
    "mape_gb_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_rf_p = round((np.mean(np.abs(x_val_p['pred_rf'] - x_val_p['act']) / x_val_p['act']))*100,2)\n",
    "mape_rf_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DecisionTree MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_dt_p = round((np.mean(np.abs(x_val_p['pred_dt'] - x_val_p['act']) / x_val_p['act']))*100,2)\n",
    "mape_dt_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LASSO MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ls_p = round((np.mean(np.abs(x_val_p['pred_ls'] - x_val_p['act']) / x_val_p['act']))*100,2)\n",
    "mape_ls_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('test_actual_price.csv')\n",
    "sample = sample.sort_values(by='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb_c = xgb_model_complete.predict(data_test[features].values)\n",
    "pred_gb_c = gb_model_complete.predict(data_test[features].values)\n",
    "pred_rf_c = rf_model_complete.predict(data_test[features].values)\n",
    "pred_ls_c = ls_model_complete.predict(data_test[features].values)\n",
    "pred_dt_c = dt_model_complete.predict(data_test[features].values)\n",
    "sample['pred_xgb'] = np.expm1(pred_xgb_c)\n",
    "sample['pred_gb'] = np.expm1(pred_gb_c)\n",
    "sample['pred_rf'] = np.expm1(pred_rf_c)\n",
    "sample['pred_ls'] = np.expm1(pred_ls_c)\n",
    "sample['pred_dt'] = np.expm1(pred_dt_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_xgb_c = round((np.mean(np.abs(sample['SalePrice'] - sample['pred_xgb']) / sample['SalePrice']))*100,2)\n",
    "mape_xgb_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boost MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_gb_c = round((np.mean(np.abs(sample['SalePrice'] - sample['pred_gb']) / sample['SalePrice']))*100,2)\n",
    "mape_gb_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_rf_c = round((np.mean(np.abs(sample['SalePrice'] - sample['pred_rf']) / sample['SalePrice']))*100,2)\n",
    "mape_rf_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DecisionTree MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_dt_c = round((np.mean(np.abs(sample['SalePrice'] - sample['pred_dt']) / sample['SalePrice']))*100,2)\n",
    "mape_dt_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LASSO MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ls_c = round((np.mean(np.abs(sample['SalePrice'] - sample['pred_ls']) / sample['SalePrice']))*100,2)\n",
    "mape_ls_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['ense'] = ((mape_xgb_c*sample['pred_xgb'])+(mape_gb_c*sample['pred_gb'])+(mape_rf_c*sample['pred_rf']))/(mape_xgb_c+mape_gb_c+mape_rf_c)\n",
    "mape_en_c = round((np.mean(np.abs(sample['SalePrice'] - sample['ense']) / sample['SalePrice']))*100,2)\n",
    "mape_en_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = sample[['Id','pred_gb']]\n",
    "final.columns = ['Id','prediction']\n",
    "final.to_csv('final_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes\n",
    "\n",
    "1. Some columns have missing values of more than 15%. Imputing such columns will lead to false predictions.\n",
    "   Hence such columns are removed.\n",
    "   \n",
    "2. Columns which are integer/float with missing values of less than 15% are imputed with MissForest imputation algorithm.\n",
    "   It is a Random Forest based imputation algorithm.\n",
    "   \n",
    "3. Columns which are categorical with missing values of less than 15% are imputed with Mode.\n",
    "\n",
    "4. Then the categorical columns are converted to dummies for modelling purposes.\n",
    "\n",
    "5. The train data is split into two parts actual train and validation.\n",
    "   The validation dataset is generated to get a sense of the test data.\n",
    "\n",
    "6. Without feature engineering or feature selection the bagging and boosted models are built to get a sense of the MAPE values.\n",
    "   The MAPE values for XGBoost, GradientBoost and RandomForest without hyperparameter tuning were 18%,15%,20% appxon validation\n",
    "   \n",
    "7. The continuous features and the target feature were not normally distributed. To get make the data normally distributed for\n",
    "   better predictions, log transformation of continuous and target feature was applied. This decreased the MAPE by 3%\n",
    "   \n",
    "8. Since all the features were used for predcition, the model might overfit and the predcitions might not be proper. Hence we \n",
    "   used RFECV (Random Feature Elimination Cross Validation) algorithm to eliminate the least import features and avoid      \n",
    "   overfitting issues. The features reduced from 269 to 149 and the MAPE value improved significantly from around 13% to 10.5%.\n",
    "\n",
    "9. Now after feature engineering and feature selection, we did hyperparameter tuning and GradeintBoost model gave the least\n",
    "   MAPE of 8.3.\n",
    "   \n",
    "10. Though Random Forest model outperformed Gradient Boost model on few occassions on the test set, it is not conistent with \n",
    "    its performance of validation set where GradientBoost model was consisent and trustworthy with its predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
